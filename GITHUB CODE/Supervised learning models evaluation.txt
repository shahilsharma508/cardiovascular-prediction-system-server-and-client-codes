#Shahil Sharma
#A00152049
#code used to create and evaluate supervised learning models and find the best parameters for each model.
#run each function at a time.
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection importGridSearchCV
from sklearn.metrics import classification_report


def KNN(df):
    from sklearn.neighbors import KNeighborsClassifier
    features = ['age','sex','sbp','chol','fbs','smoking status']
    X = df[features]
    y = df['risk level']
    #dividing data into training and testing sets 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    parameters = {'n_neighbours': [3,4,5,6,7,8],'metric': ['manhattan', 'euclidean']}
    clf = GridSearchCV(KNeighborsClassifier(),parameters, cv =10, n_jobs=1)
    clf.fit(X_train,y_train)
    print('Best n_neighbours: ', clf.best_estimator_.get_params()['n_neighbours'])
    print('Best Metric: ', clf.best_estimator_.get_params()['metric'])
    CV_Result = clf.cv_results_
    print("Cross validation results: ",CV_Result)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test,y_pred))
    
def decision_tree(df):
    from sklearn.tree import DecisionTreeClassifier
    features = ['age','sex','sbp','chol','fbs','smoking status']
    X = df[features]
    y = df['risk level']
    #dividing data into training and testing sets 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    parameters = {'criterion': ['gini','entropy']}
    clf = GridSearchCV(DecisionTreeClassifier(),parameters, cv =10, n_jobs=1)
    clf.fit(X_train,y_train)
    print('Best criterion: ', clf.best_estimator_.get_params()['criterion'])
    CV_Result = clf.cv_results_
    print("Cross validation results: ",CV_Result)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test,y_pred))


def random_forrest(df):
    from sklearn.ensemble import RandomForestClassifier
    features = ['age','sex','sbp','chol','fbs','smoking status']
    X = df[features]
    y = df['risk level']
    #dividing data into training and testing sets 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    parameters = {'n_estimators': [100,200,300,400],'criterion': ['gini','entropy']}
    clf = GridSearchCV(RandomForestClassifier(),parameters, cv =10, n_jobs=1)
    clf.fit(X_train,y_train)
    print('Best criterion: ', clf.best_estimator_.get_params()['criterion'])
    print('Best n_estimators: ', clf.best_estimator_.get_params()['n_estimators'])
    CV_Result = clf.cv_results_
    print("Cross validation results: ",CV_Result)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test,y_pred))
    
def GaussianNaïveBayes(df):
    from sklearn.naive_bayes import GaussianNB
    features = ['age','sex','sbp','chol','fbs','smoking status']
    X = df[features]
    y = df['risk level']
    #dividing data into training and testing sets 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    clf = GridSearchCV(GaussianNB(), cv =10, n_jobs=1)
    clf.fit(X_train,y_train)
    print("Cross validation results: ",CV_Result)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test,y_pred))
                                                        
                                                        
def MultinomialNaïveBayes(df):
    from sklearn.naive_bayes import MultinomialNB
    features = ['age','sex','sbp','chol','fbs','smoking status']
    X = df[features]
    y = df['risk level']
    #dividing data into training and testing sets 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    clf = GridSearchCV(MultinomialNB(), cv =10, n_jobs=1)
    clf.fit(X_train,y_train)
    print("Cross validation results: ",CV_Result)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test,y_pred))
    
def BernoulliNaïveBayes(df):
    from sklearn.naive_bayes import BernoulliNB
    features = ['age','sex','sbp','chol','fbs','smoking status']
    X = df[features]
    y = df['risk level']
    #dividing data into training and testing sets 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    clf = GridSearchCV(BernoulliNB(), cv =10, n_jobs=1)
    clf.fit(X_train,y_train)
    print("Cross validation results: ",CV_Result)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test,y_pred))
    

def SVM(df):
    from sklearn.svm import SVC
    features = ['age','sex','sbp','chol','fbs','smoking status']
    X = df[features]
    y = df['risk level']
    #dividing data into training and testing sets 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    parameters = {'kernel': ['rbf','poly','sigmoid']}
    clf = GridSearchCV(SVC(),parameters, cv =10, n_jobs=1)
    clf.fit(X_train,y_train)
    print('Best kernal: ', clf.best_estimator_.get_params()['kernel'])
    CV_Result = clf.cv_results_
    print("Cross validation results: ",CV_Result)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test,y_pred))


df = pd.read_csv('data.csv')


